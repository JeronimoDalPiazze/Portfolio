# ğŸ›¢ï¸ Crude_Oil_Exports
## End-to-end data engineering project to process and analyze Canadaâ€™s crude oil export data, built entirely with open-source tools and a modern ELT architecture.

---
## ğŸš€ Overview

This project demonstrates a complete **data engineering pipeline**, including:

1. **Ingestion** â†’ raw CSV files are automatically downloaded from the [*Canada Energy Regulator*](https://open.canada.ca/data/en/dataset/0b7bf4b3-423a-45d0-a92b-e69be0b81ce4)  open data portal using **Python + pandas**  
2. **Transformation** â†’ data cleaning, typing, and modeling using **dbt Core**  
3. **Storage** â†’ local **DuckDB** database  
4. **Orchestration** â†’ handled by **Prefect** (optional flow)  
5. **Visualization** â†’ interactive dashboards built in **Metabase**

The goal is to simulate a modern data-engineering workflow â€” from ingestion to visualization â€” using open-source technologies.

---
## ğŸ§© Tech Stack

| Stage | Tool | Purpose |
|-------|------|----------|
| Ingestion | Python + pandas + requests | Automated download of CSVs from the CER open data portal |
| Storage | DuckDB | Local analytical database |
| Transformation | dbt Core | SQL-based modeling and testing |
| Orchestration | Prefect | Workflow automation |
| Visualization | Metabase | BI dashboards and data exploration |

---

## ğŸ§± Project Structure

```graphql
Crude_Oil_Exports/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/                  # ğŸ—‚ï¸ Raw CSV files automatically downloaded
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ ingest_csvs.py        # ğŸ Python script to download and validate data
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ dbt_project.yml       # Main dbt configuration
â”‚   â”œâ”€â”€ profiles/             # Connection settings
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ staging/          # Cleans and types raw data
â”‚       â””â”€â”€ marts/            # Analytical models ready for dashboards
â”œâ”€â”€ warehouse/                # ğŸ¦† DuckDB database file
â”œâ”€â”€ prefect/                  # âš™ï¸ Prefect orchestration flows
â”œâ”€â”€ metabase/                 # ğŸ“Š Dashboard configuration and Docker setup
â”œâ”€â”€ docs/                     # ğŸ“„ Technical documentation
â”œâ”€â”€ .env / .env.example       # Environment variables
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                 # This file
```
---
## âš™ï¸ How to Run Locally

```graphql
# 1. Clone the repository
git clone https://github.com/JeronimoDalPiazze/Portfolio.git
cd Portfolio/Crude_Oil_Exports

# 2. Create the virtual environment
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# 3. Create the .env file
cp .env.example .env

# 4. Run the data ingestion script
python scripts/ingest_csvs.py
# â†’ This script will download all required CSVs from the CER portal
#   and save them to data/raw/

# 5. Run dbt
set -a; source .env; set +a
dbt build --project-dir dbt
```