# ğŸ›¢ï¸ Crude_Oil_Exports

[![Python](https://img.shields.io/badge/Python-3.12-blue?logo=python)](https://www.python.org/)
[![dbt Core](https://img.shields.io/badge/dbt_Core-1.10-orange?logo=dbt)](https://docs.getdbt.com/)
[![DuckDB](https://img.shields.io/badge/DuckDB-1.1.3-yellow?logo=duckdb)](https://duckdb.org/)
[![Prefect](https://img.shields.io/badge/Prefect-3.x-blueviolet?logo=prefect)](https://www.prefect.io/)
[![Metabase](https://img.shields.io/badge/Metabase-Latest-brightgreen?logo=metabase)](https://www.metabase.com/)

End-to-end data engineering project to process and analyze Canadaâ€™s crude oil export data â€” built entirely with open-source tools and a modern ELT architecture.

## ğŸ“š Table of Contents
- [ğŸš€ Overview](#-overview)
- [ğŸ§© Tech Stack](#-tech-stack)
- [ğŸ§± Project Structure](#-project-structure)
- [âš™ï¸ How to Run Locally](#ï¸-how-to-run-locally)
- [ğŸ“Š Example Outputs](#-example-outputs)

## ğŸš€ Overview

This project replicates a real-world **data engineering pipeline**, covering all stages from raw data ingestion to analytical visualization.  
It leverages open-source technologies to simulate a modern, production-style workflow.

### Pipeline Stages

1. **Ingestion** â†’ raw CSV files are automatically downloaded from the [*Canada Energy Regulator*](https://open.canada.ca/data/en/dataset/0b7bf4b3-423a-45d0-a92b-e69be0b81ce4) open data portal using **Python + pandas**.  
2. **Transformation** â†’ data cleaning, typing, and modeling using **dbt Core**.  
3. **Storage** â†’ local **DuckDB** analytical database.  
4. **Orchestration** â†’ managed with **Prefect** (optional flow).  
5. **Visualization** â†’ interactive dashboards built with **Metabase**.

The goal is to demonstrate how raw, unstructured data can be transformed into reliable analytical assets in a maintainable, reproducible way.

## ğŸ§© Tech Stack

| Stage | Tool | Purpose |
|-------|------|----------|
| Ingestion | ğŸ Python + pandas + requests | Automated download of CSVs from the CER open data portal |
| Storage | ğŸ¦† DuckDB | Local analytical OLAP database |
| Transformation | dbt Core | SQL-based modeling, testing, and documentation |
| Orchestration | Prefect | Workflow automation and scheduling |
| Visualization | Metabase | BI dashboards and data exploration |

## ğŸ§± Project Structure

```graphql
Crude_Oil_Exports/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/                  # ğŸ—‚ï¸ Raw CSV files automatically downloaded
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ ingest_csvs.py        # ğŸ Python script to download and validate data
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ dbt_project.yml       # Main dbt configuration
â”‚   â”œâ”€â”€ profiles/             # Connection settings
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ staging/          # Cleans and types raw data
â”‚       â””â”€â”€ marts/            # Analytical models ready for dashboards
â”œâ”€â”€ warehouse/                # ğŸ¦† DuckDB database file
â”œâ”€â”€ prefect/                  # âš™ï¸ Prefect orchestration flows (to be implemented)
â”œâ”€â”€ metabase/                 # ğŸ“Š Dashboard configuration and Docker setup
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md       # Pipeline overview and diagrams
â”‚   â”œâ”€â”€ ingestion.md          # Details about the ingestion process
â”‚   â””â”€â”€ environment.md        # Local setup and configuration guide
â”œâ”€â”€ .env / .env.example       # Environment variables
â”œâ”€â”€ .gitignore                # Ignored files (raw data, cache, etc.)
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ README.md                 # This file
```

## âš™ï¸ How to Run Locally   
```bash
# 1. Clone the repository
git clone https://github.com/JeronimoDalPiazze/Portfolio.git
cd Portfolio/Crude_Oil_Exports

# 2. Create and activate the virtual environment
python -m venv .venv

# On Linux/Mac:
source .venv/bin/activate

# On Windows:
.venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Create the .env file
cp .env.example .env

# 5. Run the data ingestion script
python scripts/ingest_csvs.py
# â†’ This script downloads all required CSVs from the CER portal
#   and saves them to data/raw/

# Example output:
# â¬‡ï¸  Downloading crude-oil-exports-by-type-monthly.csv ...
# âœ… crude-oil-exports-by-type-monthly.csv: 120 rows, 7 columns
# ğŸ’¾ Saved to data/raw/crude-oil-exports-by-type-monthly.csv

# 6. Run dbt transformations
set -a; source .env; set +a
dbt build --project-dir dbt

# 7. (Optional) Launch Metabase for visualization
docker-compose -f metabase/docker-compose.yml up
```



